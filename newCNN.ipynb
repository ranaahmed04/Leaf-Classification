{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\rana ahmed\\Desktop\\Senior_2\\deepLearingproject\\Leaf-Classification\\leaf-classification\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\rana ahmed\\Desktop\\Senior_2\\deepLearingproject\\Leaf-Classification\\leaf-classification\\test.csv')\n",
    "\n",
    "\n",
    "# Concatenate train and test data for consistent processing\n",
    "combined_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Preparing classes for labeling\n",
    "traindata = combined_data[['id','species']].copy()\n",
    "traindata['id'] = traindata['id'].astype(str)\n",
    "traindata['label'] = LabelEncoder().fit_transform(traindata['species'])\n",
    "\n",
    "\n",
    "# Loading Images and Linking Labels\n",
    "images = r'C:\\Users\\rana ahmed\\Desktop\\Senior_2\\deepLearingproject\\Leaf-Classification\\leaf-classification\\images'\n",
    "\n",
    "# List comprehension to load images and link labels\n",
    "imgs = [Image.open(os.path.join(images, i)).convert('1') \n",
    "        for i in sorted(os.listdir(images)) \n",
    "        if i.split('.')[0] in traindata['id'].values]\n",
    "\n",
    "# Matching labels using list comprehension\n",
    "labels = [traindata[traindata['id'] == i.split('.')[0]]['label'].values[0] \n",
    "          for i in sorted(os.listdir(images)) \n",
    "          if i.split('.')[0] in traindata['id'].values]\n",
    "\n",
    "# Displaying information\n",
    "print(f\"We have {len(imgs)} images in the dataset\")\n",
    "print(f\"Dataset have {len(np.unique(labels))} labels \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_arrays(directory):\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Filter by file extensions\n",
    "            img = Image.open(os.path.join(directory, filename))\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "    return images\n",
    "\n",
    "images_as_array = load_images_as_arrays(images)\n",
    "print(len(images_as_array))  # Check the number of loaded images\n",
    "print(images_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_and_greyscale(images_array, new_size):\n",
    "    resized_images = []\n",
    "    for img in images_array:\n",
    "        pil_img = Image.fromarray(img)\n",
    "        # Resize\n",
    "        resized_img = pil_img.resize(new_size)\n",
    "        # Convert to grayscale\n",
    "        grayscale_img = resized_img.convert('L')\n",
    "        resized_images.append(np.array(grayscale_img))\n",
    "    return resized_images\n",
    "\n",
    "# Resize images in the array to a new size (e.g., (64, 64)) and convert to greyscale\n",
    "resized_images = resize_images_and_greyscale(images_as_array, (64, 64))\n",
    "\n",
    "# Check the shape of the resized and grayscale images\n",
    "print(\"Shape of the first image:\", resized_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_images = np.expand_dims(resized_images, axis=-1)\n",
    "print(\"Shape of the first image:\", resized_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(resized_images, labels_np, test_size=0.2, random_state=42,stratify=labels_np)\n",
    "X_train,X_val,Y_train,Y_val= train_test_split(X_train, Y_train, test_size=0.15, random_state=42,stratify=Y_train)\n",
    "\n",
    "print(f\"Data have {len(X_train)} images for training\")\n",
    "print(f\"Data have {len(X_test)} images for testing\")\n",
    "print(f\"Data have {len(X_val)} images for validatio\")\n",
    "print(f\"Training data consist of  {len(np.unique(Y_train))} classes \")\n",
    "print(f\"Testing data consist of {len(np.unique(Y_test))} classes\")\n",
    "print(f\"Validating data consist of {len(np.unique(Y_val))} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "print(f\"length of train dataset is {len(train_dataset)}\")\n",
    "print(f\"length of validation dataset is {len(validation_dataset)}\")\n",
    "print(f\"length of Testing dataset is {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the datasets\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(len(X_val)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(f\"length of train dataset is {len(train_dataset)}\")\n",
    "print(f\"length of validation dataset is {len(validation_dataset)}\")\n",
    "print(f\"length of Testing dataset is {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model using Keras Functional API\n",
    "Model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(64, 64, 1)), \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3),activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(512, (3, 3),activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def train_model(Train_dataset,Validation_dataset, batch_size, optimizer, learning_rate, weight_decay,num_epochs):\n",
    "\n",
    "    Model1\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate, decay=weight_decay)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate, decay=weight_decay)\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    Model1.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    #def scheduler(epoch, lr):\n",
    "    #   return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    #lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = Model1.fit(Train_dataset, epochs=num_epochs,validation_data=Validation_dataset, batch_size=batch_size, callbacks=[early_stopping])\n",
    "        #print(f\"{len(Train_dataset)} [{batch_size}]: loss: {history.history['loss'][0]:.4f} - accuracy: {history.history['accuracy'][0]:.4f} - val_loss: {history.history['val_loss'][0]:.4f} - val_accuracy: {history.history['val_accuracy'][0]:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result for adam optimizer,learning_rate=0.001,epoch =30, weight_decay =0.001')\n",
    "\n",
    "trial1 = train_model(train_dataset,validation_dataset,16,'Adam',0.001,0.001,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(25,10))\n",
    "ax[0].plot(trial1.history[\"loss\"],label=\"Training Loss\")\n",
    "ax[0].plot(trial1.history[\"val_loss\"],label=\"Validation Loss\")\n",
    "ax[0].set_title(\"Loss Plot\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(trial1.history[\"accuracy\"],label=\"Training accuracy\")\n",
    "ax[1].plot(trial1.history[\"val_accuracy\"],label=\"Validation accuracy\")\n",
    "ax[1].set_title(\"Accuracy Plot\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(model, testing_data):\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(testing_data, verbose=2)\n",
    "    print('Test loss: {loss:.2f}'.format(loss=test_loss))\n",
    "    print('Test accuracy: {acc:.2f}%'.format(acc=test_acc*100))\n",
    "    return\n",
    "\n",
    "test_results(Model1,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(model, testing_data):\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(testing_data, verbose=2)\n",
    "    print('Test loss: {loss:.2f}'.format(loss=test_loss))\n",
    "    print('Test accuracy: {acc:.2f}%'.format(acc=test_acc*100))\n",
    "    return\n",
    "\n",
    "test_results(Model1,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(model, testing_data):\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(testing_data, verbose=2)\n",
    "    print('Test loss: {loss:.2f}'.format(loss=test_loss))\n",
    "    print('Test accuracy: {acc:.2f}%'.format(acc=test_acc*100))\n",
    "    return\n",
    "\n",
    "test_results(Model1,validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model using Keras Functional API\n",
    "Model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(64, 64, 1)), \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3),activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(100, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def train_model(Train_dataset,Validation_dataset, batch_size, optimizer, learning_rate, weight_decay,num_epochs):\n",
    "\n",
    "    Model2\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate, decay=weight_decay)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate, decay=weight_decay)\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    Model2.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    #def scheduler(epoch, lr):\n",
    "    #   return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    #lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = Model2.fit(Train_dataset, epochs=num_epochs,validation_data=Validation_dataset, batch_size=batch_size, callbacks=[early_stopping])\n",
    "        #print(f\"{len(Train_dataset)} [{batch_size}]: loss: {history.history['loss'][0]:.4f} - accuracy: {history.history['accuracy'][0]:.4f} - val_loss: {history.history['val_loss'][0]:.4f} - val_accuracy: {history.history['val_accuracy'][0]:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result for adam optimizer,learning_rate=0.001,epoch =30, weight_decay =0.001')\n",
    "\n",
    "trial2 = train_model(train_dataset,validation_dataset,16,'Adam',0.001,0.001,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(25,10))\n",
    "ax[0].plot(trial2.history[\"loss\"],label=\"Training Loss\")\n",
    "ax[0].plot(trial2.history[\"val_loss\"],label=\"Validation Loss\")\n",
    "ax[0].set_title(\"Loss Plot\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(trial2.history[\"accuracy\"],label=\"Training accuracy\")\n",
    "ax[1].plot(trial2.history[\"val_accuracy\"],label=\"Validation accuracy\")\n",
    "ax[1].set_title(\"Accuracy Plot\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results(model, testing_data):\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(testing_data, verbose=2)\n",
    "    print('Test loss: {loss:.2f}'.format(loss=test_loss))\n",
    "    print('Test accuracy: {acc:.2f}%'.format(acc=test_acc*100))\n",
    "    return\n",
    "\n",
    "test_results(Model2,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
